{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with the Keras sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import IPython\n",
    "from six.moves import urllib\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set common constants\n",
    "this_repo_url = 'https://github.com/CLynie/Lynie_tensorflow-tutorial/blob/master/'\n",
    "this_tutorial_url = this_repo_url + 'Classification_of_MNIST '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset containss 70,000 grayscale images of handwritten digits at a resolution of 28 by 28 pixels. The task is to take one of these images as input and predict the most likely digit contained in the image (along with a relative confidence in this prediction):\n",
    "\n",
    "<img src=\"https://i.imgur.com/ITrm9x4.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "Now, we load the dataset. The images are 28x28 NumPy arrays, with pixel values ranging between 0 and 255. The *labels* are an array of integers, ranging from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# reshape images to specify that it's a single channel\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We <b> scale these values </b> to a range of <b> 0 to 1 </b> before feeding to the neural network model. For this, we divide the values by 255. It's important that the **_training set_** and the **_testing set_** are preprocessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_images(imgs): # should work for both a single image and multiple images\n",
    "    sample_img = imgs if len(imgs.shape) == 2 else imgs[0]\n",
    "    assert sample_img.shape in [(28, 28, 1), (28, 28)], sample_img.shape # make sure images are 28x28 and single-channel (grayscale)\n",
    "    return imgs / 255.0\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "print(np.shape(train_images))\n",
    "test_images = preprocess_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    rd = random.randrange(train_images.shape[0]-5)\n",
    "    plt.imshow(train_images[i+rd].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i+rd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# 32 convolution filters used each of size 3x3\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=tf.nn.tanh, input_shape=(28, 28, 1)))\n",
    "# 64 convolution filters used each of size 3x3\n",
    "model.add(Conv2D(64, (3, 3), activation=tf.nn.tanh))\n",
    "# choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# randomly turn neurons on and off to improve convergence\n",
    "model.add(Dropout(0.25))\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model.add(Flatten())\n",
    "# fully connected to get all relevant data\n",
    "model.add(Dense(128, activation=tf.nn.tanh))\n",
    "# one more dropout\n",
    "model.add(Dropout(0.5))\n",
    "# output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2068 - acc: 0.9365\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1157 - acc: 0.9641\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.1068 - acc: 0.9661\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1009 - acc: 0.9683\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.0962 - acc: 0.9704\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "10000/10000 [==============================] - 3s 278us/sample - loss: 0.0609 - acc: 0.9814\n",
      "Test accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
