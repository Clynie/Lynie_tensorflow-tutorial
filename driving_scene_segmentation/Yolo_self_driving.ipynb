{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yad2k.models'; 'yad2k' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38c8c0720514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolo_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_colors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myad2k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_yolo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myolo_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_boxes_to_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_true_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Python/Github/Lynie_tensorflow-tutorial/driving_scene_segmentation/yad2k.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from yad2k.models.keras_yolo import (space_to_depth_x2,\n\u001b[0m\u001b[1;32m     26\u001b[0m                                      space_to_depth_x2_output_shape)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yad2k.models'; 'yad2k' is not a package"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Conv2D\n",
    "from keras.models import load_model, Model\n",
    "print(tf.__version__)\n",
    "from yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes\n",
    "from yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_head(feats, anchors, num_classes):\n",
    "    \"\"\"Convert final layer features to bounding box parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feats : tensor\n",
    "        Final convolutional layer features.\n",
    "    anchors : array-like\n",
    "        Anchor box widths and heights.\n",
    "    num_classes : int\n",
    "        Number of target classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    box_xy : tensor\n",
    "        x, y box predictions adjusted by spatial location in conv layer.\n",
    "    box_wh : tensor\n",
    "        w, h box predictions adjusted by anchors and conv spatial resolution.\n",
    "    box_conf : tensor\n",
    "        Probability estimate for whether each box contains any object.\n",
    "    box_class_pred : tensor\n",
    "        Probability distribution estimate for each box over class labels.\n",
    "    \"\"\"\n",
    "    num_anchors = len(anchors)\n",
    "    # Reshape to batch, height, width, num_anchors, box_params.\n",
    "    anchors_tensor = K.reshape(K.variable(anchors), [1, 1, 1, num_anchors, 2])\n",
    "    # Static implementation for fixed models.\n",
    "    # TODO: Remove or add option for static implementation.\n",
    "    # _, conv_height, conv_width, _ = K.int_shape(feats)\n",
    "    # conv_dims = K.variable([conv_width, conv_height])\n",
    "\n",
    "    # Dynamic implementation of conv dims for fully convolutional model.\n",
    "    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n",
    "    # In YOLO the height index is the inner most iteration.\n",
    "    conv_height_index = K.arange(0, stop=conv_dims[0])\n",
    "    conv_width_index = K.arange(0, stop=conv_dims[1])\n",
    "    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n",
    "\n",
    "    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n",
    "    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n",
    "    conv_width_index = K.tile(K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n",
    "    conv_width_index = K.flatten(K.transpose(conv_width_index))\n",
    "    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n",
    "    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n",
    "    conv_index = K.cast(conv_index, K.dtype(feats))\n",
    "    \n",
    "    feats = K.reshape(feats, [-1, conv_dims[0], conv_dims[1], num_anchors, num_classes + 5])\n",
    "    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n",
    "\n",
    "    # Static generation of conv_index:\n",
    "    # conv_index = np.array([_ for _ in np.ndindex(conv_width, conv_height)])\n",
    "    # conv_index = conv_index[:, [1, 0]]  # swap columns for YOLO ordering.\n",
    "    # conv_index = K.variable(\n",
    "    #     conv_index.reshape(1, conv_height, conv_width, 1, 2))\n",
    "    # feats = Reshape(\n",
    "    #     (conv_dims[0], conv_dims[1], num_anchors, num_classes + 5))(feats)\n",
    "\n",
    "    box_confidence = K.sigmoid(feats[..., 4:5])\n",
    "    box_xy = K.sigmoid(feats[..., :2])\n",
    "    box_wh = K.exp(feats[..., 2:4])\n",
    "    box_class_probs = K.softmax(feats[..., 5:])\n",
    "\n",
    "    # Adjust preditions to each spatial grid point and anchor size.\n",
    "    # Note: YOLO iterates over height index before width index.\n",
    "    box_xy = (box_xy + conv_index) / conv_dims\n",
    "    box_wh = box_wh * anchors_tensor / conv_dims\n",
    "\n",
    "    return box_confidence, box_xy, box_wh, box_class_probs\n",
    "\n",
    "\n",
    "def yolo_boxes_to_corners(box_xy, box_wh):\n",
    "    \"\"\"Convert YOLO box predictions to bounding box corners.\"\"\"\n",
    "    box_mins = box_xy - (box_wh / 2.)\n",
    "    box_maxes = box_xy + (box_wh / 2.)\n",
    "\n",
    "    return K.concatenate([\n",
    "        box_mins[..., 1:2],  # y_min\n",
    "        box_mins[..., 0:1],  # x_min\n",
    "        box_maxes[..., 1:2],  # y_max\n",
    "        box_maxes[..., 0:1]  # x_max\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "def yolo_eval(yolo_outputs,\n",
    "              image_shape,\n",
    "              max_boxes=10,\n",
    "              score_threshold=.6,\n",
    "              iou_threshold=.5):\n",
    "    \"\"\"Evaluate YOLO model on given input batch and return filtered boxes.\"\"\"\n",
    "    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs\n",
    "    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
    "    boxes, scores, classes = yolo_filter_boxes(\n",
    "        box_confidence, boxes, box_class_probs, threshold=score_threshold)\n",
    "    \n",
    "    # Scale boxes back to original image shape.\n",
    "    height = image_shape[0]\n",
    "    width = image_shape[1]\n",
    "    image_dims = K.stack([height, width, height, width])\n",
    "    image_dims = K.reshape(image_dims, [1, 4])\n",
    "    boxes = boxes * image_dims\n",
    "\n",
    "    # TODO: Something must be done about this ugly hack!\n",
    "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')\n",
    "    K.get_session().run(tf.variables_initializer([max_boxes_tensor]))\n",
    "    nms_index = tf.image.non_max_suppression(\n",
    "        boxes, scores, max_boxes_tensor, iou_threshold=iou_threshold)\n",
    "    boxes = K.gather(boxes, nms_index)\n",
    "    scores = K.gather(scores, nms_index)\n",
    "    classes = K.gather(classes, nms_index)\n",
    "    \n",
    "    return boxes, scores, classes\n",
    "\n",
    "\n",
    "def preprocess_true_boxes(true_boxes, anchors, image_size):\n",
    "    \"\"\"Find detector in YOLO where ground truth box should appear.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes : array\n",
    "        List of ground truth boxes in form of relative x, y, w, h, class.\n",
    "        Relative coordinates are in the range [0, 1] indicating a percentage\n",
    "        of the original image dimensions.\n",
    "    anchors : array\n",
    "        List of anchors in form of w, h.\n",
    "        Anchors are assumed to be in the range [0, conv_size] where conv_size\n",
    "        is the spatial dimension of the final convolutional features.\n",
    "    image_size : array-like\n",
    "        List of image dimensions in form of h, w in pixels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    detectors_mask : array\n",
    "        0/1 mask for detectors in [conv_height, conv_width, num_anchors, 1]\n",
    "        that should be compared with a matching ground truth box.\n",
    "    matching_true_boxes: array\n",
    "        Same shape as detectors_mask with the corresponding ground truth box\n",
    "        adjusted for comparison with predicted parameters at training time.\n",
    "    \"\"\"\n",
    "    height, width = image_size\n",
    "    num_anchors = len(anchors)\n",
    "    # Downsampling factor of 5x 2-stride max_pools == 32.\n",
    "    # TODO: Remove hardcoding of downscaling calculations.\n",
    "    assert height % 32 == 0, 'Image sizes in YOLO_v2 must be multiples of 32.'\n",
    "    assert width % 32 == 0, 'Image sizes in YOLO_v2 must be multiples of 32.'\n",
    "    conv_height = height // 32\n",
    "    conv_width = width // 32\n",
    "    num_box_params = true_boxes.shape[1]\n",
    "    detectors_mask = np.zeros(\n",
    "        (conv_height, conv_width, num_anchors, 1), dtype=np.float32)\n",
    "    matching_true_boxes = np.zeros(\n",
    "        (conv_height, conv_width, num_anchors, num_box_params),\n",
    "        dtype=np.float32)\n",
    "\n",
    "    for box in true_boxes:\n",
    "        # scale box to convolutional feature spatial dimensions\n",
    "        box_class = box[4:5]\n",
    "        box = box[0:4] * np.array(\n",
    "            [conv_width, conv_height, conv_width, conv_height])\n",
    "        i = np.floor(box[1]).astype('int')\n",
    "        j = min(np.floor(box[0]).astype('int'),1)\n",
    "        best_iou = 0\n",
    "        best_anchor = 0\n",
    "                \n",
    "        for k, anchor in enumerate(anchors):\n",
    "            # Find IOU between box shifted to origin and anchor box.\n",
    "            box_maxes = box[2:4] / 2.\n",
    "            box_mins = -box_maxes\n",
    "            anchor_maxes = (anchor / 2.)\n",
    "            anchor_mins = -anchor_maxes\n",
    "\n",
    "            intersect_mins = np.maximum(box_mins, anchor_mins)\n",
    "            intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n",
    "            intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "            intersect_area = intersect_wh[0] * intersect_wh[1]\n",
    "            box_area = box[2] * box[3]\n",
    "            anchor_area = anchor[0] * anchor[1]\n",
    "            iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_anchor = k\n",
    "                \n",
    "        if best_iou > 0:\n",
    "            detectors_mask[i, j, best_anchor] = 1\n",
    "            adjusted_box = np.array(\n",
    "                [\n",
    "                    box[0] - j, box[1] - i,\n",
    "                    np.log(box[2] / anchors[best_anchor][0]),\n",
    "                    np.log(box[3] / anchors[best_anchor][1]), box_class\n",
    "                ],\n",
    "                dtype=np.float32)\n",
    "            matching_true_boxes[i, j, best_anchor] = adjusted_box\n",
    "    return detectors_mask, matching_true_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
